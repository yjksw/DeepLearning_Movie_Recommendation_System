{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# 패키지 import\n",
    "from pathlib import Path\n",
    "import pandas as pd\n",
    "import numpy as np\n",
    "\n",
    "data = pd.read_csv('./ml-latest-small/ratings.csv')\n",
    "# data.head()\n",
    "print(data.shape)\n",
    "\n",
    "# 먼저 데이터를 Train과 Validation데이터로 나눈다\n",
    "np.random.seed(3)\n",
    "msk = np.random.rand(len(data)) < 0.8\n",
    "# train = data.copy()\n",
    "train = data[msk].copy()\n",
    "val = data[~msk].copy()\n",
    "test = pd.read_csv('./ml-25m/ratings.csv')\n",
    "\n",
    "print(train.head())\n",
    "print(train.shape)\n",
    "print(val.head())\n",
    "print(val.shape)\n",
    "print(test.head())\n",
    "print(test.shape)\n",
    "\n",
    "# 다음은 Pandas의 컬럼을 범주형의 id로 인코드해주는 함수이다\n",
    "def proc_col(col, train_col=None):\n",
    "    \"\"\" Encodes a pandas column with continous ids. \"\"\"\n",
    "    # Unique한 row를 찾는다 즉 사용자 혹은 영화이다\n",
    "    if train_col is not None:\n",
    "        uniq = train_col.unique()\n",
    "#         print(uniq)\n",
    "    else:\n",
    "        uniq = col.unique()\n",
    "    # 사용자/영화를 인덱스와 매핑해준다\n",
    "    name2idx = {o:i for i,o in enumerate(uniq)}\n",
    "    # 그리고 그것을 포맷팅해서 리턴한다\n",
    "    return name2idx, np.array([name2idx.get(x, -1) for x in col]), len(uniq)\n",
    "\n",
    "# 다음은 실제로 데이터를 인코딩으로 만들어주는 함수이다\n",
    "# 위에서 정의해준 proc_col을 사용한다\n",
    "def encode_data(df, train=None):\n",
    "    \"\"\" Encodes rating data with continous user and movie ids.\n",
    "    If train is provided, encodes df with the same encoding as train.\n",
    "    \"\"\"\n",
    "    df = df.copy()\n",
    "    for col_name in [\"userId\", \"movieId\"]:\n",
    "        train_col = None\n",
    "        if train is not None:\n",
    "            train_col = train[col_name]\n",
    "        n2i,col,len_uniq = proc_col(df[col_name], train_col)\n",
    "        #n2i는 트레인 기준으로 만드는거임\n",
    "        df[col_name] = col\n",
    "        df = df[df[col_name] >= 0]\n",
    "    return df\n",
    "\n",
    "# encoding the train and validation data\n",
    "print(\"Train Encoding\")\n",
    "df_train = encode_data(train)\n",
    "print(\"Valid Encoding\")\n",
    "df_val = encode_data(val, train)\n",
    "print(\"Test Encoding\")\n",
    "df_test = encode_data(test, train)\n",
    "print(df_test)\n",
    "\n",
    "num_users = len(df_train.userId.unique())\n",
    "num_items = len(df_train.movieId.unique())\n",
    "\n",
    "import torch\n",
    "import torch.nn as nn\n",
    "import torch.nn.functional as F\n",
    "\n",
    "def validation_loss(model, unsqueeze=False):\n",
    "    model.eval()\n",
    "    users = torch.LongTensor(df_val.userId.values)\n",
    "    items = torch.LongTensor(df_val.movieId.values)\n",
    "    ratings = torch.FloatTensor(df_val.rating.values)\n",
    "    if unsqueeze:\n",
    "        ratings = ratings.unsqueeze(1)\n",
    "    y_hat = model(users, items)\n",
    "    print(\"y_hat :\", y_hat, \",\", y_hat.shape)\n",
    "    loss = F.mse_loss(y_hat, ratings)\n",
    "    print(\"validation loss {:.3f}\".format(loss.item()))\n",
    "\n",
    "def train_mf(model, epochs=10, lr=0.01, wd=0.0, unsqueeze=False):\n",
    "    optimizer = torch.optim.Adam(model.parameters(), lr=lr, weight_decay=wd)\n",
    "    model.train()\n",
    "    print(\"Train,\", \"Learning Rate :\", lr)\n",
    "    for i in range(epochs):\n",
    "        users = torch.LongTensor(df_train.userId.values)\n",
    "        items = torch.LongTensor(df_train.movieId.values)\n",
    "        ratings = torch.FloatTensor(df_train.rating.values)\n",
    "        if unsqueeze:\n",
    "            ratings = ratings.unsqueeze(1)\n",
    "        y_hat = model(users, items)\n",
    "        loss = F.mse_loss(y_hat, ratings)\n",
    "        optimizer.zero_grad()\n",
    "        loss.backward()\n",
    "        optimizer.step()\n",
    "        if i%5 == 4:\n",
    "            print(\"epoch\", i+1, \":\", loss.item())\n",
    "    print(\"Valid\")\n",
    "    validation_loss(model, unsqueeze)\n",
    "\n",
    "def test(model, userId=1, size=5):\n",
    "    print(\"Test\")\n",
    "    model.eval()\n",
    "    global df_test\n",
    "    \n",
    "    #특정 사용자 추출 \n",
    "    target_usr = df_test['userId'] == userId\n",
    "    target_df = df_test[target_usr]\n",
    "    users = torch.LongTensor(target_df.userId.values)\n",
    "    items = torch.LongTensor(target_df.movieId.values)\n",
    "    #ratings = torch.FloatTensor(df_test.rating.values)\n",
    "    y_hat = model(users, items)\n",
    "\n",
    "    #y_hat(tensor)를 predictions(pandas dataframe)으로 변환\n",
    "    predictions = y_hat.detach().numpy()\n",
    "    predictions = pd.DataFrame(predictions)\n",
    "    predictions.columns = [\"predictions\"]\n",
    "\n",
    "    #df_test(dataframe)에 predictions 붙이기 => new_df\n",
    "    #df_test.index = [i for i in range(len(df_test))]\n",
    "    df_test = df_test.reset_index(drop=True)\n",
    "    new_df = pd.concat([df_test, predictions], axis=1)\n",
    "    print()\n",
    "    print(\"new_df\")\n",
    "    print(new_df)\n",
    "\n",
    "    #movie 이름 붙이기\n",
    "    movies = pd.read_csv('./ml-latest-small/movies.csv')\n",
    "    new_df = pd.merge(new_df, movies, on='movieId')\n",
    "    print()\n",
    "    print(\"movie name\")\n",
    "    print(new_df)\n",
    "\n",
    "    #predictions로 sort\n",
    "    new_df = new_df.sort_values(by=\"predictions\", ascending=False)\n",
    "    print()\n",
    "    print(\"sort\")\n",
    "    print(new_df)\n",
    "\n",
    "    #size만큼만 남기기\n",
    "    new_df = new_df[:size]\n",
    "    print()\n",
    "    print(\"size\")\n",
    "    print(new_df)\n",
    "\n",
    "    #결과 출력\n",
    "    result = []\n",
    "    #new_df.index = [i for i in range(len(new_df))]\n",
    "    new_df = new_df.reset_index(drop=True)\n",
    "    print()\n",
    "    print(\"Movie Recommendation for User\", userId)\n",
    "    for i in range(len(new_df)):\n",
    "        result.append(new_df[\"title\"][i])\n",
    "        print(i+1, \":\", result[i])\n",
    "\n",
    "class NNCollabFiltering(nn.Module):\n",
    "    def __init__(self, num_users, num_items, emb_size=100, n_hidden=10):\n",
    "        super(NNCollabFiltering, self).__init__()\n",
    "        self.user_emb = nn.Embedding(num_users, emb_size)\n",
    "        self.item_emb = nn.Embedding(num_items, emb_size)\n",
    "        self.lin1 = nn.Linear(emb_size*2, n_hidden)\n",
    "        self.lin2 = nn.Linear(n_hidden, 1)\n",
    "        self.drop1 = nn.Dropout(0.1)\n",
    "\n",
    "    def forward(self, u, v):\n",
    "        U = self.user_emb(u)\n",
    "        V = self.item_emb(v)\n",
    "        x = F.relu(torch.cat([U, V], dim=1))\n",
    "        x = self.drop1(x)\n",
    "        x = F.relu(self.lin1(x))\n",
    "        x = self.lin2(x)\n",
    "        return x\n",
    "\n",
    "# model = NNCollabFiltering(num_users, num_items, emb_size=100)\n",
    "# train_mf(model, epochs=50, lr=0.01, wd=1e-6, unsqueeze=True)\n",
    "\n",
    "model = NNCollabFiltering(num_users, num_items, emb_size=100)\n",
    "train_mf(model, epochs=30, lr=0.05, wd=1e-6, unsqueeze=True)\n",
    "\n",
    "# model = NNCollabFiltering(num_users, num_items, emb_size=100)\n",
    "# train_mf(model, epochs=50, lr=0.001, wd=1e-6, unsqueeze=True)\n",
    "\n",
    "test(model, 5, 10)"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.7.4"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
